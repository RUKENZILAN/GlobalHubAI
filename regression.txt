Regression

Hello again. You already know the basic principle of how machines learn. Now, we are ready to dive deeper into specific machine learning algorithms and how we use them in practice. Machine learning is traditionally divided into two main categories: supervised and unsupervised learning. We will start with supervised learning and one of its techniques: regression.

First, what do we mean by supervised machine learning? It is an approach where a computer algorithm is trained using input data and the expected output. In other words, we know the answer and have nicely labeled it as well. This is why we can also call the expected output “labels” or “ground truth”. In unsupervised learning, on the other hand, the training data is not labeled by humans in advance and the machine has to come up with meaningful answers, but we’ll cover this later. Now, let’s dig deeper into supervised learning. You have input variables, which we will call x, and labels which we will call y. And we use an algorithm to learn the mapping function from the input x to the output y. This means we want our algorithm to learn to compute output y when a certain input x is given.

Regression is a type of supervised learning that uses an algorithm to understand the relationship between a dependent variable, that is the input, and an independent variable, which is the output. Regression models are helpful for predicting numerical values based on different features’ values. For example, temperature forecast based on wind, humidity and pressure, or price estimations of a car based on its model year, brand, and transmission type. In regression, we want to build a relationship between each feature and the output so that we can predict for example, the price of the house when we know the features but not the price. If this relationship is linear, this algorithm is called linear regression.

Linear regression is perhaps the most well-known and well-understood algorithm in statistics and machine learning. Here is an example dataset based on two variables. A simple linear regression model tries to explain the relationship between the two variables using a best fitting straight line. We call this a regression line. The independent variable X is used to predict the value of the dependent variable which is represented as Y. In our example, X is rainfall measured in millimeters and Y is the number of umbrellas sold.

We want to find out the equation of the line: Y = wX + b.

Here, w is the slope of the gradient of the line. It indicates how much the variable X impacts Y, which is why we call it the “weight”. And b is the value of Y when there is no X or the X is zero, this is called the “bias”. We have X, the rainfall measurements, and if we find the weight and the bias, we can say that we have found the regression line and we are able to predict the number of umbrellas, or Y, for each new rainfall. There are many possible strategies to calculate the regression line; the most popular one is the least squares. We start by drawing a line to represent this relationship. Then we measure the distances between the line and each datapoint, the residuals. We sum up the residuals, then adjust the weight and the bias to minimize total residual to find the best line. This was simple linear regression because there was only one feature therefore, only one weight. When we have multiple features, it’s called multiple linear regression. Let’s work on multiple linear regression with a new example.

We have a dataset for predicting house prices. In this dataset, there are multiple features like the number of bedrooms, age of the building, covered area and so on. These features are our inputs, our X’s. We also have the sale price of the house in the dataset. This is our output, y. However, our task is to predict this output variable using the features.

We want to find the equation of this line: Y = w1X1 + w2X2 + …and so on + b.

Here, X are the features, the w are the weights associated to each feature, and b is the bias. Now, let’s say we have trained a model and weights and bias are learned. But how do we know if the model predictions are correct and not some random numbers thrown by the model? We need to evaluate the performance of this regression model and for this purpose, we use performance evaluation metrics.

One of the most commonly used evaluation metrics is just taking the difference between predicted and actual value of some test points. But this difference can be positive or negative and if we compute the total difference, the positive and negative will cancel out each other. To solve this problem, we square all the differences. Now all the differences are positive but squared. Then, we take the mean of these values. This value is called the Mean Squared Error, MSE. To better understand how big the error is, we take the square root of it and get the Root Mean Squared Error RMSE. This RMSE is in the same unit as the predictions. This will all come together in a practical example!

The first step is reading the data. To do that, we need to import Python’s handy data science library, Pandas. After importing the pandas library we can easily load our train and test datasets using read_csv. We will use the train dataset to help our regression model to learn some important patterns in the data. Then we’ll use the test dataset to check how well the model learned the patterns or how well it predicts. Let’s start with this simple operation. Let’s observe features in our train dataset. We can see that some of the features we have include the house’s general quality, the year it was built, the size of the garage, and so on.

Now that we have our datasets loaded, we can import linear regression models from the most important machine learning library of Python; sklearn. It is an open-source library for machine learning. There are many models constructed in this library, we just need to import the one that we will use. After importing linear regression model, we can assign it to the “model” variable to use it easily. In this example, we’re trying to predict the house prices. The column we want to predict also is called “ground truth”, “target” or “labels” and other columns are called “features” or “attributes”. For the model to predict the house prices, first, we need to define which column has the house prices, or the ground truth. Then we remove it from the features of the data using the drop function and assign it as ‘labels’ using the loc function. Inside the drop function, we write the column name followed by the argument axis which we set to 1. This indicates that the specified column needs to be deleted. Basically, we want to predict y with the help of x. And generally, we assign target to the y variable, features to the X variable. Then we can fit our model, which means teaching the hidden patterns in the training dataset into it. After the fitting process, our model is almost ready to make predictions. But before that, we also need to divide our test dataset into “target” and “features”.

Here the target dataset contains the actual values which our model will compare its predictions. Now, we are completely ready to make predictions using the features of the test dataset. Let’s observe our predictions and actual test values. For that, we can simply put them into the same data frame and observe some of the rows using head and tail functions. We see that the actual values and the predictions by our model are more or less close. Of course, as in every machine learning model, there are some inaccuracies, but we will gradually reduce those and increase the accuracy of the model. Don’t worry, this is just your first model! Of course, comparing some data points with your eyes won’t tell you how well your model predicts. This is exactly where we use evaluation metrics! Let’s import mean squared error from the sklearn library. We also need to import square root from the NumPy library, because we want to observe our root mean squared error as it’s in the same unit with our data. After importing, we can use these functions to average the calculated error.

Our RMSE is approximately 33 000! If we consider our average price is 185 000, and maximum price as big as 500 000, then 33 000 may be considered normal for your first model. Keep going! Also, we can check which features have the most impact on our predictions. Basically, we can check for correlations on our train dataset. But since we need correlations between target and features, we can simply take the “SalePrice” column from this data frame. From the data frame, we can’t decide which ones have the most impact. Let’s sort and see the top 10 using sort_values, then head functions. Don’t forget that we need to set ascending as false because we want to see 10 highest values!

Here, we can see the features that have the strongest relation to the target variable. Overall material and finish of the house, ground living area square feet, and garage size in car capacity are the top three features that have the strongest correlation with house sale price. This means they have the biggest impact on predicting it. You can also see that the first feature in this list is the sale price of the house, with a correlation of 1. That is normal, because the correlation of a variable with itself is 1. This is all for now. In this video we learned about supervised learning, regression, and linear regression. We trained a house price prediction model using a real-world dataset. The Colab notebook is available in the course materials. Feel free to play with this model by changing its parameters or features.

Well, while this is a very useful model for some types of problems, for the most part the world is not linear. Think about a typical tennis player. In the beginning of her career, she was learning. Then, her performance was at its peak and there was no one like her. With time, she aged, and her performance dropped. We cannot model a linear regression to calculate the performance of a sportsperson because this is a non-linear problem, and the linear regression model works well for linear equations. But don’t worry, there are other models we can use for nonlinear problems. See you in the next video!

